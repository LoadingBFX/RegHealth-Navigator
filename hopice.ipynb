{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# âœ… Fix OpenMP crash (OMP: Error #15)\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import os\n",
    "import chromadb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\pauls\\AppData\\Local\\Temp\\ipykernel_25772\\3675702670.py:2: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  file_path = \"data\\Hospice Text.xlsx\"\n"
     ]
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = \"data\\Hospice Text.xlsx\"\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Generate embeddings for text using BAAI/bge-large-en-v1.5.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512, overlap=128):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"hospice_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Add of existing embedding ID: 0_chunk0\n",
      "Add of existing embedding ID: 0_chunk1\n",
      "Add of existing embedding ID: 0_chunk2\n",
      "Add of existing embedding ID: 0_chunk3\n",
      "Add of existing embedding ID: 0_chunk4\n",
      "Add of existing embedding ID: 0_chunk5\n",
      "Add of existing embedding ID: 0_chunk6\n",
      "Add of existing embedding ID: 0_chunk7\n",
      "Add of existing embedding ID: 0_chunk8\n",
      "Add of existing embedding ID: 0_chunk9\n",
      "Add of existing embedding ID: 0_chunk10\n",
      "Add of existing embedding ID: 0_chunk11\n",
      "Add of existing embedding ID: 1_chunk0\n",
      "Add of existing embedding ID: 1_chunk1\n",
      "Add of existing embedding ID: 1_chunk2\n",
      "Add of existing embedding ID: 1_chunk3\n",
      "Add of existing embedding ID: 1_chunk4\n",
      "Add of existing embedding ID: 1_chunk5\n",
      "Add of existing embedding ID: 2_chunk0\n",
      "Add of existing embedding ID: 2_chunk1\n",
      "Add of existing embedding ID: 2_chunk2\n",
      "Add of existing embedding ID: 2_chunk3\n",
      "Add of existing embedding ID: 2_chunk4\n",
      "Add of existing embedding ID: 2_chunk5\n",
      "Add of existing embedding ID: 2_chunk6\n",
      "Add of existing embedding ID: 3_chunk0\n",
      "Add of existing embedding ID: 3_chunk1\n",
      "Add of existing embedding ID: 3_chunk2\n",
      "Add of existing embedding ID: 3_chunk3\n",
      "Add of existing embedding ID: 3_chunk4\n",
      "Add of existing embedding ID: 3_chunk5\n",
      "Add of existing embedding ID: 4_chunk0\n",
      "Add of existing embedding ID: 4_chunk1\n",
      "Add of existing embedding ID: 4_chunk2\n",
      "Add of existing embedding ID: 4_chunk3\n",
      "Add of existing embedding ID: 5_chunk0\n",
      "Add of existing embedding ID: 5_chunk1\n",
      "Add of existing embedding ID: 6_chunk0\n",
      "Add of existing embedding ID: 6_chunk1\n",
      "Add of existing embedding ID: 7_chunk0\n",
      "Add of existing embedding ID: 7_chunk1\n",
      "Add of existing embedding ID: 8_chunk0\n",
      "Add of existing embedding ID: 8_chunk1\n",
      "Add of existing embedding ID: 9_chunk0\n",
      "Add of existing embedding ID: 0_chunk0\n",
      "Add of existing embedding ID: 0_chunk1\n",
      "Add of existing embedding ID: 0_chunk2\n",
      "Add of existing embedding ID: 0_chunk3\n",
      "Add of existing embedding ID: 0_chunk4\n",
      "Add of existing embedding ID: 0_chunk5\n",
      "Add of existing embedding ID: 0_chunk6\n",
      "Add of existing embedding ID: 0_chunk7\n",
      "Add of existing embedding ID: 0_chunk8\n",
      "Add of existing embedding ID: 0_chunk9\n",
      "Add of existing embedding ID: 0_chunk10\n",
      "Add of existing embedding ID: 0_chunk11\n",
      "Add of existing embedding ID: 1_chunk0\n",
      "Add of existing embedding ID: 1_chunk1\n",
      "Add of existing embedding ID: 1_chunk2\n",
      "Add of existing embedding ID: 1_chunk3\n",
      "Add of existing embedding ID: 1_chunk4\n",
      "Add of existing embedding ID: 1_chunk5\n",
      "Add of existing embedding ID: 2_chunk0\n",
      "Add of existing embedding ID: 2_chunk1\n",
      "Add of existing embedding ID: 2_chunk2\n",
      "Add of existing embedding ID: 2_chunk3\n",
      "Add of existing embedding ID: 2_chunk4\n",
      "Add of existing embedding ID: 2_chunk5\n",
      "Add of existing embedding ID: 2_chunk6\n",
      "Add of existing embedding ID: 3_chunk0\n",
      "Add of existing embedding ID: 3_chunk1\n",
      "Add of existing embedding ID: 3_chunk2\n",
      "Add of existing embedding ID: 3_chunk3\n",
      "Add of existing embedding ID: 3_chunk4\n",
      "Add of existing embedding ID: 3_chunk5\n",
      "Add of existing embedding ID: 4_chunk0\n",
      "Add of existing embedding ID: 4_chunk1\n",
      "Add of existing embedding ID: 4_chunk2\n",
      "Add of existing embedding ID: 4_chunk3\n",
      "Add of existing embedding ID: 5_chunk0\n",
      "Add of existing embedding ID: 5_chunk1\n",
      "Add of existing embedding ID: 6_chunk0\n",
      "Add of existing embedding ID: 6_chunk1\n",
      "Add of existing embedding ID: 7_chunk0\n",
      "Add of existing embedding ID: 7_chunk1\n",
      "Add of existing embedding ID: 8_chunk0\n",
      "Add of existing embedding ID: 8_chunk1\n",
      "Add of existing embedding ID: 9_chunk0\n",
      "Add of existing embedding ID: 0_chunk0\n",
      "Add of existing embedding ID: 0_chunk1\n",
      "Add of existing embedding ID: 0_chunk2\n",
      "Add of existing embedding ID: 0_chunk3\n",
      "Add of existing embedding ID: 0_chunk4\n",
      "Add of existing embedding ID: 0_chunk5\n",
      "Add of existing embedding ID: 0_chunk6\n",
      "Add of existing embedding ID: 0_chunk7\n",
      "Add of existing embedding ID: 0_chunk8\n",
      "Add of existing embedding ID: 0_chunk9\n",
      "Add of existing embedding ID: 0_chunk10\n",
      "Add of existing embedding ID: 0_chunk11\n",
      "Add of existing embedding ID: 1_chunk0\n",
      "Add of existing embedding ID: 1_chunk1\n",
      "Add of existing embedding ID: 1_chunk2\n",
      "Add of existing embedding ID: 1_chunk3\n",
      "Add of existing embedding ID: 1_chunk4\n",
      "Add of existing embedding ID: 1_chunk5\n",
      "Add of existing embedding ID: 2_chunk0\n",
      "Add of existing embedding ID: 2_chunk1\n",
      "Add of existing embedding ID: 2_chunk2\n",
      "Add of existing embedding ID: 2_chunk3\n",
      "Add of existing embedding ID: 2_chunk4\n",
      "Add of existing embedding ID: 2_chunk5\n",
      "Add of existing embedding ID: 2_chunk6\n",
      "Add of existing embedding ID: 3_chunk0\n",
      "Add of existing embedding ID: 3_chunk1\n",
      "Add of existing embedding ID: 3_chunk2\n",
      "Add of existing embedding ID: 3_chunk3\n",
      "Add of existing embedding ID: 3_chunk4\n",
      "Add of existing embedding ID: 3_chunk5\n",
      "Add of existing embedding ID: 4_chunk0\n",
      "Add of existing embedding ID: 4_chunk1\n",
      "Add of existing embedding ID: 4_chunk2\n",
      "Add of existing embedding ID: 4_chunk3\n",
      "Add of existing embedding ID: 5_chunk0\n",
      "Add of existing embedding ID: 5_chunk1\n",
      "Add of existing embedding ID: 6_chunk0\n",
      "Add of existing embedding ID: 6_chunk1\n",
      "Add of existing embedding ID: 7_chunk0\n",
      "Add of existing embedding ID: 7_chunk1\n",
      "Add of existing embedding ID: 8_chunk0\n",
      "Add of existing embedding ID: 8_chunk1\n",
      "Add of existing embedding ID: 9_chunk0\n",
      "Add of existing embedding ID: 0_chunk0\n",
      "Add of existing embedding ID: 0_chunk1\n",
      "Add of existing embedding ID: 0_chunk2\n",
      "Add of existing embedding ID: 0_chunk3\n",
      "Add of existing embedding ID: 0_chunk4\n",
      "Add of existing embedding ID: 0_chunk5\n",
      "Add of existing embedding ID: 0_chunk6\n",
      "Add of existing embedding ID: 0_chunk7\n",
      "Add of existing embedding ID: 0_chunk8\n",
      "Add of existing embedding ID: 0_chunk9\n",
      "Add of existing embedding ID: 0_chunk10\n",
      "Add of existing embedding ID: 0_chunk11\n",
      "Add of existing embedding ID: 1_chunk0\n",
      "Add of existing embedding ID: 1_chunk1\n",
      "Add of existing embedding ID: 1_chunk2\n",
      "Add of existing embedding ID: 1_chunk3\n",
      "Add of existing embedding ID: 1_chunk4\n",
      "Add of existing embedding ID: 1_chunk5\n",
      "Add of existing embedding ID: 2_chunk0\n",
      "Add of existing embedding ID: 2_chunk1\n",
      "Add of existing embedding ID: 2_chunk2\n",
      "Add of existing embedding ID: 2_chunk3\n",
      "Add of existing embedding ID: 2_chunk4\n",
      "Add of existing embedding ID: 2_chunk5\n",
      "Add of existing embedding ID: 2_chunk6\n",
      "Add of existing embedding ID: 3_chunk0\n",
      "Add of existing embedding ID: 3_chunk1\n",
      "Add of existing embedding ID: 3_chunk2\n",
      "Add of existing embedding ID: 3_chunk3\n",
      "Add of existing embedding ID: 3_chunk4\n",
      "Add of existing embedding ID: 3_chunk5\n",
      "Add of existing embedding ID: 4_chunk0\n",
      "Add of existing embedding ID: 4_chunk1\n",
      "Add of existing embedding ID: 4_chunk2\n",
      "Add of existing embedding ID: 4_chunk3\n",
      "Add of existing embedding ID: 5_chunk0\n",
      "Add of existing embedding ID: 5_chunk1\n",
      "Add of existing embedding ID: 6_chunk0\n",
      "Add of existing embedding ID: 6_chunk1\n",
      "Add of existing embedding ID: 7_chunk0\n",
      "Add of existing embedding ID: 7_chunk1\n",
      "Add of existing embedding ID: 8_chunk0\n",
      "Add of existing embedding ID: 8_chunk1\n",
      "Add of existing embedding ID: 9_chunk0\n",
      "Add of existing embedding ID: 0_chunk0\n",
      "Add of existing embedding ID: 0_chunk1\n",
      "Add of existing embedding ID: 0_chunk2\n",
      "Add of existing embedding ID: 0_chunk3\n",
      "Add of existing embedding ID: 0_chunk4\n",
      "Add of existing embedding ID: 0_chunk5\n",
      "Add of existing embedding ID: 0_chunk6\n",
      "Add of existing embedding ID: 0_chunk7\n",
      "Add of existing embedding ID: 0_chunk8\n",
      "Add of existing embedding ID: 0_chunk9\n",
      "Add of existing embedding ID: 0_chunk10\n",
      "Add of existing embedding ID: 0_chunk11\n",
      "Add of existing embedding ID: 1_chunk0\n",
      "Add of existing embedding ID: 1_chunk1\n",
      "Add of existing embedding ID: 1_chunk2\n",
      "Add of existing embedding ID: 1_chunk3\n",
      "Add of existing embedding ID: 1_chunk4\n",
      "Add of existing embedding ID: 1_chunk5\n",
      "Add of existing embedding ID: 2_chunk0\n",
      "Add of existing embedding ID: 2_chunk1\n",
      "Add of existing embedding ID: 2_chunk2\n",
      "Add of existing embedding ID: 2_chunk3\n",
      "Add of existing embedding ID: 2_chunk4\n",
      "Add of existing embedding ID: 2_chunk5\n",
      "Add of existing embedding ID: 2_chunk6\n",
      "Add of existing embedding ID: 3_chunk0\n",
      "Add of existing embedding ID: 3_chunk1\n",
      "Add of existing embedding ID: 3_chunk2\n",
      "Add of existing embedding ID: 3_chunk3\n",
      "Add of existing embedding ID: 3_chunk4\n",
      "Add of existing embedding ID: 3_chunk5\n",
      "Add of existing embedding ID: 4_chunk0\n",
      "Add of existing embedding ID: 4_chunk1\n",
      "Add of existing embedding ID: 4_chunk2\n",
      "Add of existing embedding ID: 4_chunk3\n",
      "Add of existing embedding ID: 5_chunk0\n",
      "Add of existing embedding ID: 5_chunk1\n",
      "Add of existing embedding ID: 6_chunk0\n",
      "Add of existing embedding ID: 6_chunk1\n",
      "Add of existing embedding ID: 7_chunk0\n",
      "Add of existing embedding ID: 7_chunk1\n",
      "Add of existing embedding ID: 8_chunk0\n",
      "Add of existing embedding ID: 8_chunk1\n",
      "Add of existing embedding ID: 9_chunk0\n",
      "Insert of existing embedding ID: 0_chunk0\n",
      "Add of existing embedding ID: 0_chunk0\n",
      "Insert of existing embedding ID: 0_chunk1\n",
      "Add of existing embedding ID: 0_chunk1\n",
      "Insert of existing embedding ID: 0_chunk2\n",
      "Add of existing embedding ID: 0_chunk2\n",
      "Insert of existing embedding ID: 0_chunk3\n",
      "Add of existing embedding ID: 0_chunk3\n",
      "Insert of existing embedding ID: 0_chunk4\n",
      "Add of existing embedding ID: 0_chunk4\n",
      "Insert of existing embedding ID: 0_chunk5\n",
      "Add of existing embedding ID: 0_chunk5\n",
      "Insert of existing embedding ID: 0_chunk6\n",
      "Add of existing embedding ID: 0_chunk6\n",
      "Insert of existing embedding ID: 0_chunk7\n",
      "Add of existing embedding ID: 0_chunk7\n",
      "Insert of existing embedding ID: 0_chunk8\n",
      "Add of existing embedding ID: 0_chunk8\n",
      "Insert of existing embedding ID: 0_chunk9\n",
      "Add of existing embedding ID: 0_chunk9\n",
      "Insert of existing embedding ID: 0_chunk10\n",
      "Add of existing embedding ID: 0_chunk10\n",
      "Insert of existing embedding ID: 0_chunk11\n",
      "Add of existing embedding ID: 0_chunk11\n",
      " 10%|â–ˆ         | 1/10 [00:18<02:44, 18.33s/it]Insert of existing embedding ID: 1_chunk0\n",
      "Add of existing embedding ID: 1_chunk0\n",
      "Insert of existing embedding ID: 1_chunk1\n",
      "Add of existing embedding ID: 1_chunk1\n",
      "Insert of existing embedding ID: 1_chunk2\n",
      "Add of existing embedding ID: 1_chunk2\n",
      "Insert of existing embedding ID: 1_chunk3\n",
      "Add of existing embedding ID: 1_chunk3\n",
      "Insert of existing embedding ID: 1_chunk4\n",
      "Add of existing embedding ID: 1_chunk4\n",
      "Insert of existing embedding ID: 1_chunk5\n",
      "Add of existing embedding ID: 1_chunk5\n",
      " 20%|â–ˆâ–ˆ        | 2/10 [00:26<01:37, 12.23s/it]Insert of existing embedding ID: 2_chunk0\n",
      "Add of existing embedding ID: 2_chunk0\n",
      "Insert of existing embedding ID: 2_chunk1\n",
      "Add of existing embedding ID: 2_chunk1\n",
      "Insert of existing embedding ID: 2_chunk2\n",
      "Add of existing embedding ID: 2_chunk2\n",
      "Insert of existing embedding ID: 2_chunk3\n",
      "Add of existing embedding ID: 2_chunk3\n",
      "Insert of existing embedding ID: 2_chunk4\n",
      "Add of existing embedding ID: 2_chunk4\n",
      "Insert of existing embedding ID: 2_chunk5\n",
      "Add of existing embedding ID: 2_chunk5\n",
      "Insert of existing embedding ID: 2_chunk6\n",
      "Add of existing embedding ID: 2_chunk6\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:35<01:16, 11.00s/it]Insert of existing embedding ID: 3_chunk0\n",
      "Add of existing embedding ID: 3_chunk0\n",
      "Insert of existing embedding ID: 3_chunk1\n",
      "Add of existing embedding ID: 3_chunk1\n",
      "Insert of existing embedding ID: 3_chunk2\n",
      "Add of existing embedding ID: 3_chunk2\n",
      "Insert of existing embedding ID: 3_chunk3\n",
      "Add of existing embedding ID: 3_chunk3\n",
      "Insert of existing embedding ID: 3_chunk4\n",
      "Add of existing embedding ID: 3_chunk4\n",
      "Insert of existing embedding ID: 3_chunk5\n",
      "Add of existing embedding ID: 3_chunk5\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:43<00:58,  9.70s/it]Insert of existing embedding ID: 4_chunk0\n",
      "Add of existing embedding ID: 4_chunk0\n",
      "Insert of existing embedding ID: 4_chunk1\n",
      "Add of existing embedding ID: 4_chunk1\n",
      "Insert of existing embedding ID: 4_chunk2\n",
      "Add of existing embedding ID: 4_chunk2\n",
      "Insert of existing embedding ID: 4_chunk3\n",
      "Add of existing embedding ID: 4_chunk3\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:47<00:39,  7.81s/it]Insert of existing embedding ID: 5_chunk0\n",
      "Add of existing embedding ID: 5_chunk0\n",
      "Insert of existing embedding ID: 5_chunk1\n",
      "Add of existing embedding ID: 5_chunk1\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:50<00:24,  6.00s/it]Insert of existing embedding ID: 6_chunk0\n",
      "Add of existing embedding ID: 6_chunk0\n",
      "Insert of existing embedding ID: 6_chunk1\n",
      "Add of existing embedding ID: 6_chunk1\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:52<00:13,  4.60s/it]Insert of existing embedding ID: 7_chunk0\n",
      "Add of existing embedding ID: 7_chunk0\n",
      "Insert of existing embedding ID: 7_chunk1\n",
      "Add of existing embedding ID: 7_chunk1\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:54<00:07,  3.83s/it]Insert of existing embedding ID: 8_chunk0\n",
      "Add of existing embedding ID: 8_chunk0\n",
      "Insert of existing embedding ID: 8_chunk1\n",
      "Add of existing embedding ID: 8_chunk1\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:56<00:03,  3.17s/it]Insert of existing embedding ID: 9_chunk0\n",
      "Add of existing embedding ID: 9_chunk0\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:56<00:00,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data successfully inserted into ChromaDB with structured metadata!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert documents with chunking and metadata\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text_chunks = chunk_text(row[\"Text\"], chunk_size=512, overlap=128)\n",
    "    for chunk_id, chunk in enumerate(text_chunks):\n",
    "        embedding = get_embedding(chunk)\n",
    "        collection.add(\n",
    "            ids=[f\"{_}_chunk{chunk_id}\"],\n",
    "            embeddings=[embedding.tolist()],\n",
    "            metadatas=[{\n",
    "                \"year\": row[\"Year\"],\n",
    "                \"type\": row[\"Type\"],\n",
    "                \"section\": row[\"Section\"],\n",
    "                \"text\": chunk\n",
    "            }]\n",
    "        )\n",
    "\n",
    "print(\"âœ… Data successfully inserted into ChromaDB with structured metadata!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant text from ChromaDB\n",
    "def retrieve_text(query, limit=2):\n",
    "    \"\"\"Retrieve relevant text based on user query with improved relevance sorting.\"\"\"\n",
    "    query_embedding = get_embedding(query).tolist()\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=limit)\n",
    "    \n",
    "    if \"metadatas\" not in results or not results[\"metadatas\"]:\n",
    "        return []\n",
    "    \n",
    "    # Sort results by keyword overlap for better relevance\n",
    "    query_words = set(query.lower().split())\n",
    "    ranked_results = sorted(\n",
    "        results[\"metadatas\"][0],\n",
    "        key=lambda doc: len(set(doc[\"text\"].lower().split()) & query_words),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return [doc[\"text\"] for doc in ranked_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Falcon-7B-Instruct model from .bin using ctransformers\n",
    "FALCON_MODEL_PATH = os.path.join(\"models\", \"falcon-7b-instruct.ggccv1.q5_1.bin\")\n",
    "\n",
    "if not os.path.exists(FALCON_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model not found at {FALCON_MODEL_PATH}. Check the path and ensure the model is downloaded.\")\n",
    "\n",
    "falcon_model = AutoModelForCausalLM.from_pretrained(FALCON_MODEL_PATH, model_type=\"falcon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    \"\"\"Retrieve relevant context and pass to Falcon-7B for response generation, enforcing strict token limits.\"\"\"\n",
    "    context_data = retrieve_text(query)\n",
    "\n",
    "    if not context_data:\n",
    "        return \"No relevant information found.\"\n",
    "\n",
    "    structured_context = \"\\n---\\n\".join(context_data)\n",
    "\n",
    "    # Token Limit Enforcement\n",
    "    max_total_tokens = 512  # Falcon-7B GGML max limit\n",
    "    max_query_tokens = 50    # Reserve 50 tokens for the query\n",
    "    max_response_tokens = 100 # Ensure response fits\n",
    "    max_context_tokens = max_total_tokens - max_query_tokens - max_response_tokens\n",
    "\n",
    "    # Truncate context and query if necessary\n",
    "    context_words = structured_context.split()\n",
    "    truncated_context = \" \".join(context_words[:max_context_tokens])\n",
    "\n",
    "    query_words = query.split()\n",
    "    truncated_query = \" \".join(query_words[:max_query_tokens])\n",
    "\n",
    "    # Final prompt ensuring it fits within model's max token limit\n",
    "    prompt = f\"\"\"You are an expert assistant answering a query based on retrieved regulatory documents. \n",
    "    Always provide fact-based answers and reference the provided text when possible.\n",
    "    If the retrieved text does not answer the question, say 'Insufficient information'.\n",
    "\n",
    "    Below is the relevant information:\n",
    "    {truncated_context}\n",
    "\n",
    "    Based on this, answer the following question in a well-structured and complete manner:\n",
    "\n",
    "    Question: {truncated_query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        llm_response = falcon_model(prompt, max_new_tokens=max_response_tokens)\n",
    "        return llm_response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (607) exceeded maximum context length (512).\n",
      "Number of tokens (608) exceeded maximum context length (512).\n",
      "Number of tokens (609) exceeded maximum context length (512).\n",
      "Number of tokens (610) exceeded maximum context length (512).\n",
      "Number of tokens (611) exceeded maximum context length (512).\n",
      "Number of tokens (612) exceeded maximum context length (512).\n",
      "Number of tokens (613) exceeded maximum context length (512).\n",
      "Number of tokens (614) exceeded maximum context length (512).\n",
      "Number of tokens (615) exceeded maximum context length (512).\n",
      "Number of tokens (616) exceeded maximum context length (512).\n",
      "Number of tokens (617) exceeded maximum context length (512).\n",
      "Number of tokens (618) exceeded maximum context length (512).\n",
      "Number of tokens (619) exceeded maximum context length (512).\n",
      "Number of tokens (620) exceeded maximum context length (512).\n",
      "Number of tokens (621) exceeded maximum context length (512).\n",
      "Number of tokens (622) exceeded maximum context length (512).\n",
      "Number of tokens (623) exceeded maximum context length (512).\n",
      "Number of tokens (624) exceeded maximum context length (512).\n",
      "Number of tokens (625) exceeded maximum context length (512).\n",
      "Number of tokens (626) exceeded maximum context length (512).\n",
      "Number of tokens (627) exceeded maximum context length (512).\n",
      "Number of tokens (628) exceeded maximum context length (512).\n",
      "Number of tokens (629) exceeded maximum context length (512).\n",
      "Number of tokens (630) exceeded maximum context length (512).\n",
      "Number of tokens (631) exceeded maximum context length (512).\n",
      "Number of tokens (632) exceeded maximum context length (512).\n",
      "Number of tokens (633) exceeded maximum context length (512).\n",
      "Number of tokens (634) exceeded maximum context length (512).\n",
      "Number of tokens (635) exceeded maximum context length (512).\n",
      "Number of tokens (636) exceeded maximum context length (512).\n",
      "Number of tokens (637) exceeded maximum context length (512).\n",
      "Number of tokens (638) exceeded maximum context length (512).\n",
      "Number of tokens (639) exceeded maximum context length (512).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1,) =type;\n",
      "User:  a a is an certain individuals can be used to, as the.poster in the.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Compare hospice wage indexes from 2024 and 2025.\"\n",
    "response = generate_response(user_query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
