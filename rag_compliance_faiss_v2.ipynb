{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import textwrap\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "# === SETUP ===\n",
    "load_dotenv()  # Load variables from .env\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# === LOAD & CLEAN EXCEL DATA ===\n",
    "df = pd.read_excel(\"data/Hospice Text.xlsx\")\n",
    "df = df[['Year', 'Type', 'Section', 'Text']].dropna()\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['doc_id'] = df.index\n",
    "\n",
    "# === TAG EXTRACTION ===\n",
    "def extract_tags(section):\n",
    "    tags = []\n",
    "    if \"Final\" in section:\n",
    "        tags.append(\"Final\")\n",
    "    elif \"Proposed\" in section:\n",
    "        tags.append(\"Proposed\")\n",
    "    year_match = re.search(r\"FY\\s?(\\d{4})\", section)\n",
    "    if year_match:\n",
    "        tags.append(year_match.group(1))\n",
    "    keyword_matches = re.findall(r\"Hospice [A-Za-z ]+\", section)\n",
    "    tags += [kw.strip() for kw in keyword_matches]\n",
    "    return list(set(tags))\n",
    "\n",
    "df['tags'] = df['Section'].apply(extract_tags)\n",
    "\n",
    "# === CHUNKING ===\n",
    "def chunk_text(text, max_tokens=400):\n",
    "    max_chars = max_tokens * 4\n",
    "    return textwrap.wrap(text, width=max_chars, break_long_words=False)\n",
    "\n",
    "chunked_docs = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_text(row['Text'])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_docs.append({\n",
    "            \"doc_id\": f\"{row['doc_id']}_chunk{i}\",\n",
    "            \"year\": row[\"Year\"],\n",
    "            \"type\": row[\"Type\"],\n",
    "            \"section\": row[\"Section\"],\n",
    "            \"tags\": row[\"tags\"],\n",
    "            \"text\": f\"Section: {row['Section']}\\n\\n{chunk}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Embedding documents...\n",
      "âœ… FAISS index built with 63 chunks.\n"
     ]
    }
   ],
   "source": [
    "# === EMBEDDING + FAISS SETUP ===\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBED_MODEL,\n",
    "        input=[text]\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "embeddings = []\n",
    "\n",
    "print(\"ðŸ” Embedding documents...\")\n",
    "for doc in chunked_docs:\n",
    "    embedding = get_embedding(doc[\"text\"])\n",
    "    texts.append(doc[\"text\"])\n",
    "    metadatas.append({\n",
    "        \"doc_id\": doc[\"doc_id\"],\n",
    "        \"year\": doc[\"year\"],\n",
    "        \"type\": doc[\"type\"],\n",
    "        \"section\": doc[\"section\"],\n",
    "        \"tags\": doc[\"tags\"]\n",
    "    })\n",
    "    embeddings.append(np.array(embedding, dtype='float32'))\n",
    "\n",
    "embedding_dim = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.vstack(embeddings))\n",
    "print(f\"âœ… FAISS index built with {index.ntotal} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEARCH FUNCTION ===\n",
    "def search(query, k=10, token_limit=3000):\n",
    "    q_embedding = get_embedding(query)\n",
    "    q_vec = np.array(q_embedding, dtype='float32').reshape(1, -1)\n",
    "\n",
    "    distances, indices = index.search(q_vec, k)\n",
    "\n",
    "    results = []\n",
    "    token_count = 0\n",
    "    for i in indices[0]:\n",
    "        text = texts[i]\n",
    "        tokens = len(text) // 4\n",
    "        if token_count + tokens > token_limit:\n",
    "            break\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"metadata\": metadatas[i]\n",
    "        })\n",
    "        token_count += tokens\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GPT-4: Q&A MODE ===\n",
    "def generate_answer(query, docs):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"{d['metadata']['year']} {d['metadata']['type']} - {d['metadata']['section']}\\n{d['text']}\"\n",
    "        for d in docs\n",
    "    ])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a healthcare compliance expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Answer the following question using the retrieved documents.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GPT-4: SUGGESTION MODE ===\n",
    "def generate_recommendations(docs, context_query=None):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"{d['metadata']['year']} {d['metadata']['type']} - {d['metadata']['section']}\\n{d['text']}\"\n",
    "        for d in docs\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a regulatory compliance advisor for hospice care providers.\n",
    "\n",
    "Based on the following CMS healthcare rules, generate **practical, actionable suggestions** for a hospice agency.\n",
    "\n",
    "Explain what they should do to stay compliant, reduce risk, and prepare effectively.\n",
    "\n",
    "{f\"Focus on this question: {context_query}\" if context_query else \"\"}\n",
    "\\n\\nRegulatory Text:\\n{context}\n",
    "\n",
    "Suggestions:\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a compliance consultant for hospice care agencies.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– AI Response:\n",
      "\n",
      "1. **Stay Updated with Regulatory Changes**: CMS updates the hospice cap amount annually, so it is important to stay up-to-date with these changes. These updates are currently based on the hospice payment update percentage instead of the CPI-U.\n",
      "\n",
      "2. **Budget Adjustments**: Be prepared for changes in cap amount, and adjust the annual budget accordingly. The hospice cap amount for FY 2025 is finalized at $34,465.34. Compared to the FY 2024 cap amount of $33,494.01, there is an increase of about 2.9%. Make budget decisions based on this new cap to accommodate this increase.\n",
      "\n",
      "3. **Financial Planning**: Ensure you have an effective financial planning system to meet any increasing costs due to updates in the hospice cap amount. Building in a buffer for updates in the hospice cap, as well as any changes in the hospice payment update percentage, can mitigate potential financial pressure.\n",
      "\n",
      "4. **Quality of Care**: Despite increases in the cap, it's vital to maintain the quality, integrity, and consistency of patient care. This could potentially lead to more reimbursements and lower costs in the long run.\n",
      "\n",
      "5. **Review Reports**: Monitor aggregrate cap reports frequently to ensure you are not getting close to exceeding your cap. Once you reach your cap, you would be financially responsible for any further costs.\n",
      "\n",
      "6. **External Clinical Documentation Audit**: Regularly perform external clinical documentation audits to ensure compliance with all hospice regulatory requirements, which can help prevent overpayments that could contribute to reaching the cap.\n",
      "\n",
      "7. **Regulatory Training**: Continually train the staff on hospice regulatory requirements. It's essential that they understand these regulations and the implications of exceeding the hospice cap. \n",
      "\n",
      "8. **Compliance Program**: Implement a strong compliance program which can help to prevent unnecessary expenditure and avoid cap overages. This includes regular reviews to ensure that all patients continue to meet the eligibility requirements for hospice care. \n",
      "\n",
      "9. **Regular Policy Review**: Regularly review policy and procedures to ensure they align with the current regulatory requirements. Updates or changes should be made as needed based on regulatory changes.\n",
      "\n",
      "10. **Effective Communication**: Maintain effective communication with the CMS and attend their briefing sessions whenever possible to understand any changes to the cap or related regulations. This proactive approach will help you prepare in advance and adjust your strategies as required.\n",
      "\n",
      "In conclusion, staying abreast with CMS updates and modifying strategies accordingly can help your hospice stay within the updated cap amounts, avoiding overages and maintaining financial stability.\n"
     ]
    }
   ],
   "source": [
    "# === MAIN INTERFACE ===\n",
    "if __name__ == \"__main__\":\n",
    "    mode = input(\"Select mode: (1) Ask a question (2) Get suggestions\\n> \").strip()\n",
    "\n",
    "    query = input(\"\\nEnter your query:\\n> \")\n",
    "\n",
    "    top_docs = search(query)\n",
    "\n",
    "    if mode == \"2\":\n",
    "        output = generate_recommendations(top_docs, context_query=query)\n",
    "    else:\n",
    "        output = generate_answer(query, top_docs)\n",
    "\n",
    "    print(\"\\nðŸ¤– AI Response:\\n\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What actions should hospitals take in response to the 2025 final cap amount changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing it with the reasoning agents. \n",
    "\n",
    "# Perplexity Pro, deep research/deep reasoning\n",
    "\n",
    "# Give 3 documents, 2024 final, 2025 proposed, 2025 final, ask specifically coparision between finals, prposed vs final, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
