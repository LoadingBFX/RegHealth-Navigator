{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94489f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Generating embeddings with OpenAI using token-aware batching and splitting long chunks...\n",
      "\n",
      "ðŸ“Š Total tokens embedded: 2232358\n",
      "ðŸ’° Estimated cost: $0.2232\n",
      "âœ… FAISS index saved as rag_data/faiss.index\n",
      "âœ… Metadata saved as rag_data/faiss_metadata.json\n",
      "\n",
      "ðŸ“„ Token usage by document:\n",
      "- 2022 Hospice Final Rule.xml: 163622 tokens â‰ˆ $0.0164\n",
      "- 2022 Hospice Proposed Rule.xml: 144402 tokens â‰ˆ $0.0144\n",
      "- 2023 Hospice Final Rule.xml: 43751 tokens â‰ˆ $0.0044\n",
      "- 2023 Hospice Proposed Rule.xml: 51436 tokens â‰ˆ $0.0051\n",
      "- 2023 SNF Final Rule.xml: 465514 tokens â‰ˆ $0.0466\n",
      "- 2023 SNF Proposed Rule.xml: 332939 tokens â‰ˆ $0.0333\n",
      "- 2024 Hospice Final Rule.xml: 51436 tokens â‰ˆ $0.0051\n",
      "- 2024 SNF Final Rule.xml: 301541 tokens â‰ˆ $0.0302\n",
      "- 2025 Hospice Final Rule.xml: 96572 tokens â‰ˆ $0.0097\n",
      "- 2025 Hospice Proposed Rule.xml: 64766 tokens â‰ˆ $0.0065\n",
      "- 2025 SNF Final Rule.xml: 301541 tokens â‰ˆ $0.0302\n",
      "- 2025 SNF Proposed Rule.xml: 214838 tokens â‰ˆ $0.0215\n",
      "ðŸ’¾ Cost summary saved as rag_data/embedding_cost_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Instantiate OpenAI client\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(\"rag_data\", exist_ok=True)\n",
    "\n",
    "# Load preprocessed chunks\n",
    "with open(\"rag_data/chunks.json\", \"r\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "# Tokenizer setup for ada-002\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "MAX_TOKENS_PER_BATCH = 8191\n",
    "MAX_TOKENS_PER_CHUNK = 8191\n",
    "SAFETY_MARGIN = 50  # leave headroom\n",
    "\n",
    "# Estimate token count for a string\n",
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Split a long text into smaller parts by sentence\n",
    "def split_into_chunks(text, max_tokens):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for sentence in sentences:\n",
    "        if count_tokens(current + sentence) < max_tokens - SAFETY_MARGIN:\n",
    "            current += sentence + '. '\n",
    "        else:\n",
    "            chunks.append(current.strip())\n",
    "            current = sentence + '. '\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    return chunks\n",
    "\n",
    "# Embedding with token-aware batching and long chunk splitting\n",
    "def get_openai_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    embeddings = []\n",
    "    batch = []\n",
    "    batch_token_count = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in texts:\n",
    "        if count_tokens(text) > MAX_TOKENS_PER_CHUNK - SAFETY_MARGIN:\n",
    "            sub_chunks = split_into_chunks(text, MAX_TOKENS_PER_CHUNK)\n",
    "        else:\n",
    "            sub_chunks = [text]\n",
    "\n",
    "        for chunk in sub_chunks:\n",
    "            if not isinstance(chunk, str) or not chunk.strip():\n",
    "                print(\"âš ï¸ Skipped an invalid or empty chunk.\")\n",
    "                continue\n",
    "            tokens = count_tokens(chunk)\n",
    "            total_tokens += tokens\n",
    "            if tokens > MAX_TOKENS_PER_CHUNK - SAFETY_MARGIN:\n",
    "                print(\"âš ï¸ Chunk still too large after splitting. Truncating.\")\n",
    "                encoded = encoding.encode(chunk)\n",
    "                chunk = encoding.decode(encoded[:MAX_TOKENS_PER_CHUNK - SAFETY_MARGIN])\n",
    "                tokens = count_tokens(chunk)\n",
    "\n",
    "            if batch_token_count + tokens > MAX_TOKENS_PER_BATCH - SAFETY_MARGIN:\n",
    "                response = client.embeddings.create(input=batch, model=model)\n",
    "                embeddings.extend([r.embedding for r in response.data])\n",
    "                batch = []\n",
    "                batch_token_count = 0\n",
    "\n",
    "            batch.append(chunk)\n",
    "            batch_token_count += tokens\n",
    "\n",
    "    if batch:\n",
    "        response = client.embeddings.create(input=batch, model=model)\n",
    "        embeddings.extend([r.embedding for r in response.data])\n",
    "\n",
    "    estimated_cost = total_tokens / 1000 * 0.0001\n",
    "    print(f\"\\nðŸ“Š Total tokens embedded: {total_tokens}\")\n",
    "    print(f\"ðŸ’° Estimated cost: ${estimated_cost:.4f}\")\n",
    "\n",
    "    return embeddings, total_tokens\n",
    "\n",
    "print(\"ðŸ”„ Generating embeddings with OpenAI using token-aware batching and splitting long chunks...\")\n",
    "embeddings, total_tokens = get_openai_embeddings(texts)\n",
    "embedding_matrix = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = len(embedding_matrix[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_matrix)\n",
    "faiss.write_index(index, \"rag_data/faiss.index\")\n",
    "print(\"âœ… FAISS index saved as rag_data/faiss.index\")\n",
    "\n",
    "# Save metadata and track per-document token usage\n",
    "faiss_metadata = []\n",
    "embedding_index = 0\n",
    "token_log_by_doc = {}\n",
    "\n",
    "for chunk in chunks:\n",
    "    text = chunk[\"text\"]\n",
    "    source_file = chunk[\"metadata\"].get(\"source_file\", \"unknown\")\n",
    "    token_log_by_doc.setdefault(source_file, 0)\n",
    "\n",
    "    if count_tokens(text) > MAX_TOKENS_PER_CHUNK - SAFETY_MARGIN:\n",
    "        sub_chunks = split_into_chunks(text, MAX_TOKENS_PER_CHUNK)\n",
    "    else:\n",
    "        sub_chunks = [text]\n",
    "\n",
    "    for sub_chunk in sub_chunks:\n",
    "        if not isinstance(sub_chunk, str) or not sub_chunk.strip():\n",
    "            continue\n",
    "        if count_tokens(sub_chunk) > MAX_TOKENS_PER_CHUNK - SAFETY_MARGIN:\n",
    "            encoded = encoding.encode(sub_chunk)\n",
    "            sub_chunk = encoding.decode(encoded[:MAX_TOKENS_PER_CHUNK - SAFETY_MARGIN])\n",
    "        token_log_by_doc[source_file] += count_tokens(sub_chunk)\n",
    "        faiss_metadata.append({\n",
    "            \"text\": sub_chunk,\n",
    "            \"section_header\": chunk[\"section_header\"],\n",
    "            \"metadata\": chunk[\"metadata\"]\n",
    "        })\n",
    "        embedding_index += 1\n",
    "\n",
    "# Save metadata\n",
    "with open(\"rag_data/faiss_metadata.json\", \"w\") as f:\n",
    "    json.dump(faiss_metadata, f, indent=2)\n",
    "print(\"âœ… Metadata saved as rag_data/faiss_metadata.json\")\n",
    "\n",
    "# Print token usage per document\n",
    "print(\"\\nðŸ“„ Token usage by document:\")\n",
    "doc_costs = {}\n",
    "for doc, tokens in token_log_by_doc.items():\n",
    "    cost = tokens / 1000 * 0.0001\n",
    "    doc_costs[doc] = {\"tokens\": tokens, \"cost\": round(cost, 4)}\n",
    "    print(f\"- {doc}: {tokens} tokens â‰ˆ ${cost:.4f}\")\n",
    "\n",
    "# Save cost summary\n",
    "with open(\"rag_data/embedding_cost_summary.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"estimated_total_cost\": round(total_tokens / 1000 * 0.0001, 4),\n",
    "        \"per_document\": doc_costs\n",
    "    }, f, indent=2)\n",
    "print(\"ðŸ’¾ Cost summary saved as rag_data/embedding_cost_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
