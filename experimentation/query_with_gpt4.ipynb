{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46daaa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Embedding query: What changed in final hospice payment between 2024 and 2025?\n",
      "\n",
      "üß† Asking GPT-4...\n",
      "\n",
      "‚úÖ Answer:\n",
      "\n",
      "Several changes were made in the final hospice payment between 2024 and 2025:\n",
      "\n",
      "1. **Hospice Payment Update Percentage**: The hospice payment update percentage for FY 2025 was increased to 2.9 percent, up from the proposed 2.6 percent **[Section: III. Provisions of the Final Rule > A. Final FY 2025 Hospice Wage Index and Rate Update > 3. FY 2025 Hospice Payment Update Percentage]**. This update is based on a 3.4 percent inpatient hospital market basket percentage increase, reduced by a final 0.5 percentage point productivity adjustment **[Section: V. Regulatory Impact Analysis > C. Detailed Economic Analysis > 1. Hospice Payment Update for FY 2025]**.\n",
      "\n",
      "2. **Hospice Cap Amount**: The hospice cap amount for FY 2025 was updated by the hospice payment update percentage of 2.9 percent **[Section: III. Provisions of the Final Rule > A. Final FY 2025 Hospice Wage Index and Rate Update > 3. FY 2025 Hospice Payment Update Percentage]**. The proposed hospice cap amount for the FY 2025 cap year was $34,364.85, which is equal to the FY 2024 cap amount ($33,494.01) updated by the proposed FY 2025 hospice payment update percentage of 2.6 percent **[Section: V. Regulatory Impact Analysis > C. Detailed Economic Analysis > 1. Hospice Payment Update for FY 2025]**.\n",
      "\n",
      "3. **Wage Index Changes**: For FY 2025, a permanent 5-percent cap on wage index decreases was implemented to mitigate any potential negative impact for hospices serving beneficiaries in areas that are impacted by the proposal to adopt the revised OMB delineations **[Section: III. Provisions of the Final Rule > A. Final FY 2025 Hospice Wage Index and Rate Update > 3. FY 2025 Hospice Payment Update Percentage]**.\n",
      "\n",
      "4. **Data Collection Changes**: Implementation of a hospice patient-level item set to be used by all hospices to collect and submit standardized data on each patient admitted to hospice was required. This resulted in an annual cost burden of $184,729,739 across all hospices starting in FY 2026 **[Section: V. Regulatory Impact Analysis > C. Detailed Economic Analysis > 1. Hospice Payment Update for FY 2025]**.\n",
      "\n",
      "5. **Payment Impacts**: The FY 2025 hospice payment impacts were tabulated according to classifications such as provider type, geographic region, and facility size. The combined effects of the updated wage data and the hospice payment update percentage on FY 2025 hospice payments varied by specific types of providers and by location **[Section: V. Regulatory Impact Analysis > C. Detailed Economic Analysis > 1. Hospice Payment Update for FY 2025]**.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import openai\n",
    "import os\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Load FAISS index and metadata\n",
    "index = faiss.read_index(\"faiss.index\")\n",
    "with open(\"faiss_metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Tokenizers\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "ADA_ENCODING = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
    "\n",
    "def count_tokens(text, model_encoding=encoding):\n",
    "    return len(model_encoding.encode(text))\n",
    "\n",
    "# Embed query\n",
    "def embed_query(query):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=query\n",
    "    )\n",
    "    return np.array([response.data[0].embedding], dtype=\"float32\")\n",
    "\n",
    "# Apply filters to metadata\n",
    "def apply_filters(results, year=None, program=None, rule_type=None):\n",
    "    filtered = []\n",
    "    for item in results:\n",
    "        meta = item[\"metadata\"]\n",
    "        if year and str(meta.get(\"year\")) != str(year):\n",
    "            continue\n",
    "        if program and program.lower() not in meta.get(\"title\", \"\").lower():\n",
    "            continue\n",
    "        if rule_type and rule_type.lower() not in meta.get(\"rule_type\", \"\").lower():\n",
    "            continue\n",
    "        filtered.append(item)\n",
    "    return filtered\n",
    "\n",
    "# Build the GPT-4 prompt\n",
    "def build_prompt(query, results, prompt_token_limit=5000):\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result[\"text\"]\n",
    "        header = result[\"section_header\"]\n",
    "        next_section = f\"\\n[Section: {header}]\\n{chunk}\\n\"\n",
    "        if count_tokens(context + next_section) > prompt_token_limit:\n",
    "            break\n",
    "        context += next_section\n",
    "\n",
    "    prompt = f\"\"\"You are a regulatory analysis expert specializing in U.S. healthcare policy rules.\n",
    "\n",
    "Using the information provided below, write a comprehensive and detailed answer to the user's question.\n",
    "\n",
    "Instructions:\n",
    "- Use bullet points or numbered lists where helpful.\n",
    "- Reference section headers in square brackets (e.g., [Section: III.A.3]).\n",
    "- Highlight policy changes, comparisons, financial impacts, and dates when applicable.\n",
    "- Base your answer solely on the provided context.\n",
    "\n",
    "--- BEGIN CONTEXT ---\n",
    "{context}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "User Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Ask GPT-4 and format result\n",
    "def ask_gpt4(prompt, max_completion_tokens=2500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI that answers questions about Medicare rules.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=max_completion_tokens\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    # Inline highlighting of section headers\n",
    "    return content.replace(\"[Section:\", \"**[Section:\").replace(\"]\", \"]**\")\n",
    "\n",
    "# Main query function\n",
    "def search_and_ask(query, k=12, year=None, program=None, rule_type=None):\n",
    "    print(f\"\\nüîç Embedding query: {query}\")\n",
    "    query_embedding = embed_query(query)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    initial_results = [metadata[i] for i in indices[0]]\n",
    "\n",
    "    # Apply filters\n",
    "    results = apply_filters(initial_results, year=year, program=program, rule_type=rule_type)\n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No results found after applying filters.\")\n",
    "        return\n",
    "\n",
    "    prompt = build_prompt(query, results)\n",
    "    print(\"\\nüß† Asking GPT-4...\")\n",
    "    answer = ask_gpt4(prompt)\n",
    "    print(\"\\n‚úÖ Answer:\\n\")\n",
    "    print(answer)\n",
    "\n",
    "# Command-line runner\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        user_query = input(\"Enter your question: \")\n",
    "        year = input(\"Filter by year (or leave blank): \").strip() or None\n",
    "        program = input(\"Filter by program (Hospice, SNF, MPFS): \").strip() or None\n",
    "        rule_type = input(\"Filter by rule type (Final, Proposed): \").strip() or None\n",
    "        search_and_ask(user_query, k=12, year=year, program=program, rule_type=rule_type)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
