{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import textwrap\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "# === SETUP ===\n",
    "load_dotenv()  # Load variables from .env\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# === LOAD & CLEAN EXCEL DATA ===\n",
    "df = pd.read_excel(\"data/Hospice Text.xlsx\")\n",
    "df = df[['Year', 'Type', 'Section', 'Text']].dropna()\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['doc_id'] = df.index\n",
    "\n",
    "# === TAG EXTRACTION ===\n",
    "def extract_tags(section):\n",
    "    tags = []\n",
    "    if \"Final\" in section:\n",
    "        tags.append(\"Final\")\n",
    "    elif \"Proposed\" in section:\n",
    "        tags.append(\"Proposed\")\n",
    "    year_match = re.search(r\"FY\\s?(\\d{4})\", section)\n",
    "    if year_match:\n",
    "        tags.append(year_match.group(1))\n",
    "    keyword_matches = re.findall(r\"Hospice [A-Za-z ]+\", section)\n",
    "    tags += [kw.strip() for kw in keyword_matches]\n",
    "    return list(set(tags))\n",
    "\n",
    "df['tags'] = df['Section'].apply(extract_tags)\n",
    "\n",
    "# === CHUNKING ===\n",
    "def chunk_text(text, max_tokens=400):\n",
    "    max_chars = max_tokens * 4\n",
    "    return textwrap.wrap(text, width=max_chars, break_long_words=False)\n",
    "\n",
    "chunked_docs = []\n",
    "for _, row in df.iterrows():\n",
    "    chunks = chunk_text(row['Text'])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_docs.append({\n",
    "            \"doc_id\": f\"{row['doc_id']}_chunk{i}\",\n",
    "            \"year\": row[\"Year\"],\n",
    "            \"type\": row[\"Type\"],\n",
    "            \"section\": row[\"Section\"],\n",
    "            \"tags\": row[\"tags\"],\n",
    "            \"text\": f\"Section: {row['Section']}\\n\\n{chunk}\"\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Embedding documents...\n",
      "âœ… FAISS index built with 63 chunks.\n"
     ]
    }
   ],
   "source": [
    "# === EMBEDDING + FAISS SETUP ===\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=EMBED_MODEL,\n",
    "        input=[text]\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "embeddings = []\n",
    "\n",
    "print(\"ðŸ” Embedding documents...\")\n",
    "for doc in chunked_docs:\n",
    "    embedding = get_embedding(doc[\"text\"])\n",
    "    texts.append(doc[\"text\"])\n",
    "    metadatas.append({\n",
    "        \"doc_id\": doc[\"doc_id\"],\n",
    "        \"year\": doc[\"year\"],\n",
    "        \"type\": doc[\"type\"],\n",
    "        \"section\": doc[\"section\"],\n",
    "        \"tags\": doc[\"tags\"]\n",
    "    })\n",
    "    embeddings.append(np.array(embedding, dtype='float32'))\n",
    "\n",
    "embedding_dim = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.vstack(embeddings))\n",
    "print(f\"âœ… FAISS index built with {index.ntotal} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEARCH FUNCTION ===\n",
    "def search(query, k=10, token_limit=3000):\n",
    "    q_embedding = get_embedding(query)\n",
    "    q_vec = np.array(q_embedding, dtype='float32').reshape(1, -1)\n",
    "\n",
    "    distances, indices = index.search(q_vec, k)\n",
    "\n",
    "    results = []\n",
    "    token_count = 0\n",
    "    for i in indices[0]:\n",
    "        text = texts[i]\n",
    "        tokens = len(text) // 4\n",
    "        if token_count + tokens > token_limit:\n",
    "            break\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"metadata\": metadatas[i]\n",
    "        })\n",
    "        token_count += tokens\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"o3-mini\"  # or \"gpt-4\", or \"gpt-3.5-turbo\"\n",
    "\n",
    "# === GPT-4: Q&A MODE ===\n",
    "def generate_answer(query, docs):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"{d['metadata']['year']} {d['metadata']['type']} - {d['metadata']['section']}\\n{d['text']}\"\n",
    "        for d in docs\n",
    "    ])\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a healthcare compliance expert.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Answer the following question using the retrieved documents.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Retrieved Documents:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GPT-4: SUGGESTION MODE ===\n",
    "def generate_recommendations(docs, context_query=None):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"{d['metadata']['year']} {d['metadata']['type']} - {d['metadata']['section']}\\n{d['text']}\"\n",
    "        for d in docs\n",
    "    ])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a regulatory compliance advisor for hospice care providers.\n",
    "\n",
    "Based on the following CMS healthcare rules, generate **practical, actionable suggestions** for a hospice agency.\n",
    "\n",
    "Explain what they should do to stay compliant, reduce risk, and prepare effectively.\n",
    "\n",
    "{f\"Focus on this question: {context_query}\" if context_query else \"\"}\n",
    "\\n\\nRegulatory Text:\\n{context}\n",
    "\n",
    "Suggestions:\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a compliance consultant for hospice care agencies.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– AI Response:\n",
      "\n",
      "Below are several practical and actionable steps your hospice agency can take in response to the FY 2025 hospice cap amount changes:\n",
      "\n",
      "1. Review and Update Financial Models\n",
      "â€ƒâ€¢ Recognize that the FY 2025 hospice cap amount has been updated to $34,465.34 based on a 2.9 percent increase from the previous year. Ensure that your financial forecasts, budgets, and rate-setting models reflect this updated cap.  \n",
      "â€ƒâ€¢ Reevaluate case mix assumptions and revenue projections to account for the potential impact of the higher cap, making sure you allocate sufficient resources to manage cases that may approach or exceed the cap.\n",
      "\n",
      "2. Update Billing and Reimbursement Processes\n",
      "â€ƒâ€¢ Work with your billing and IT teams to adjust software systems and claim processing protocols so that they reflect the new cap amount. This includes altering any internal logic or rules that determine claim limits when calculating reimbursements.\n",
      "â€ƒâ€¢ Ensure that all coding, claim submission processes, and internal documentation refer to the updated cap to reduce the risk of errors or billing disputes.\n",
      "\n",
      "3. Strengthen Compliance Monitoring and Internal Controls\n",
      "â€ƒâ€¢ Establish an internal audit or compliance team review to ensure that the updates are consistently applied across all departments. This helps minimize potential noncompliance risks.\n",
      "â€ƒâ€¢ Set up alerts or tracking within your electronic health record (EHR) and financial systems so that when the aggregate for a beneficiary or service line nears the updated cap, managers are notified to review documentation and justification.\n",
      "\n",
      "4. Communicate Changes to Stakeholders\n",
      "â€ƒâ€¢ Inform key staffâ€”including finance, billing, clinical leaders, and administrationâ€”about the new FY 2025 hospice cap management requirements.  \n",
      "â€ƒâ€¢ Develop training sessions or quick-reference guides that explain how the change was determined (i.e., mandated by statute using the hospice payment update percentage) and what practical steps need to be followed in daily operations.\n",
      "â€ƒâ€¢ Engage with external consultants or legal advisors (if necessary) to review your revised billing protocols and compliance processes to ensure they align with CMSâ€™s statutory mandate.\n",
      "\n",
      "5. Monitor CMS Updates and Legislative Developments\n",
      "â€ƒâ€¢ CMS indicated that should more recent data become available between the proposal and final rule publication, they may adjust the hospice payment update percentage used. Stay engaged with CMS communications (newsletters, webinars, updates on their website) to capture any last-minute changes that could affect your cap calculations.\n",
      "â€ƒâ€¢ Periodically review regulatory bulletins and participate in industry associations where updates and best practices are shared. This will help ensure that your agency remains proactive rather than reactive.\n",
      "\n",
      "6. Prepare for Impact on Service Delivery and Operations\n",
      "â€ƒâ€¢ Assess how the updated cap might affect patient care and service delivery. For instance, if a beneficiaryâ€™s aggregate claims approach the cap, consider how care coordination or service planning may need to adjust to safeguard patientsâ€™ access to care.\n",
      "â€ƒâ€¢ Develop protocols for reviewing cases where the cap might be reached early in the service period. Such protocols can help ensure that clinical teams document extensively, justify the care provided, and communicate with payers as needed.\n",
      "\n",
      "By taking these actions, your agency can remain compliant with CMSâ€™s final rule changes, reduce the risk of billing errors or compliance issues, and ensure that your operational and financial planning is aligned with the statutory updates coming into effect.\n"
     ]
    }
   ],
   "source": [
    "# === MAIN INTERFACE ===\n",
    "if __name__ == \"__main__\":\n",
    "    mode = input(\"Select mode: (1) Ask a question (2) Get suggestions\\n> \").strip()\n",
    "\n",
    "    query = input(\"\\nEnter your query:\\n> \")\n",
    "\n",
    "    top_docs = search(query)\n",
    "\n",
    "    if mode == \"2\":\n",
    "        output = generate_recommendations(top_docs, context_query=query)\n",
    "    else:\n",
    "        output = generate_answer(query, top_docs)\n",
    "\n",
    "    print(\"\\nðŸ¤– AI Response:\\n\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What actions should hospitals take in response to the 2025 final cap amount changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing it with the reasoning agents. \n",
    "\n",
    "# Perplexity Pro, deep research/deep reasoning\n",
    "\n",
    "# Give 3 documents, 2024 final, 2025 proposed, 2025 final, ask specifically coparision between finals, prposed vs final, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
